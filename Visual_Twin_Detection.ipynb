{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJLC6qlFoRbn",
        "outputId": "d91e5ee6-0d61-431e-ccf8-81abee7417bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEq3Dj91oMvD",
        "outputId": "447c565a-dbd2-4a31-eb24-4626beb1270e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data source import complete.\n"
          ]
        }
      ],
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "ruchi798_totally_looks_like_dataset_path = kagglehub.dataset_download('ruchi798/totally-looks-like-dataset')\n",
        "\n",
        "print('Data source import complete.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_z7cpDveoMvE"
      },
      "source": [
        "![](https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/61e9d06c613399dc20011575_6NGq2uP-p-1600.png)\n",
        "\n",
        "<center><h1> üîé DoppelVision: Finding the image twin </h1></center>\n",
        "\n",
        "üå•Ô∏è Ever glanced at a cloud and saw a face? Or spotted a familiar figure in your morning toast? This phenomenon, known as pareidolia, is a fascinating quirk of the human brain, where we often see faces or familiar patterns in unrelated objects.\n",
        "\n",
        "It's this intrinsic human tendency to see familiar patterns in seemingly unrelated objects that raises an intriguing question: **Can we train machines to emulate this uniquely human trait?**\n",
        "\n",
        "ü§ñ Deep learning has made impressive advancements, but the real challenge lies in getting machines to match human perception in assessing image likeness.\n",
        "\n",
        "### Finding the right dataset\n",
        "To embark on this quest, the first step was selecting the right dataset. [The Totally-Looks-Like dataset](https://www.kaggle.com/datasets/ruchi798/totally-looks-like-dataset) emerged as an ideal choice. It's a curated collection that pairs images based on perceived similarity by humans, providing a foundation for training models to see images as we do.\n",
        "\n",
        "### Siamese Networks & Transfer Learning\n",
        "- With the dataset in place, the next phase was model selection and training. I chose to implement Siamese Networks, known for their prowess in discerning similarities, to mirror human-like image recognition. Furthermore, to optimize performance,\n",
        "- I used ResNet-50 as the backbone, to extract features from images and thereby improving the model's ability to gauge image similarities\n",
        "\n",
        "### Efficient Training & Scalability\n",
        "To handle the scale of the data scale and the complexity of the model complexity, I turned to Hugging Face's Accelerate. Paired with the Z by HP Z8 Fury, this approach ensured efficient multi-GPU training, drastically reducing training times and improving scalability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMhx5Z4VoMvF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import cv2\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Dataset, SubsetRandomSampler, random_split\n",
        "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
        "from torch.nn import SyncBatchNorm\n",
        "from torchvision.models import resnet50\n",
        "from torchvision.io import read_image\n",
        "from torchvision.transforms import ToPILImage\n",
        "from PIL import Image\n",
        "from accelerate import Accelerator\n",
        "from accelerate import notebook_launcher\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ru17uiBToMvF"
      },
      "outputs": [],
      "source": [
        "Config = {\n",
        "'TARGET_SHAPE' : (200, 200),\n",
        "'NUM_EPOCHS' : 10,\n",
        "'MARGIN': 0.5,\n",
        "'LR' : 1e-4\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scRhxodHoMvG"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed_value=12):\n",
        "    \"\"\"Set seed for reproducibility.\"\"\"\n",
        "    torch.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed_all(seed_value)  # if using multi-GPU\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    np.random.seed(seed_value)\n",
        "\n",
        "set_seed()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvfJB1KyoMvG",
        "outputId": "09d57f48-adbb-4c61-a61f-cfde9cda4711"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of left images: 2356\n",
            "\n",
            "Number of right images: 6016\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def getImagePaths(path):\n",
        "    image_names = []\n",
        "    for dirname, _, filenames in os.walk(path):\n",
        "        for filename in filenames:\n",
        "            fullpath = os.path.join(dirname, filename)\n",
        "            image_names.append(fullpath)\n",
        "    return image_names\n",
        "\n",
        "left_dir_path = \"/content/drive/MyDrive/Visual_Twin_Detection/left\"\n",
        "right_dir_path = \"/content/drive/MyDrive/Visual_Twin_Detection/right\"\n",
        "\n",
        "left_images_path = getImagePaths(left_dir_path)\n",
        "right_images_path = getImagePaths(right_dir_path)\n",
        "\n",
        "print(f\"Number of left images: {len(left_images_path)}\\n\")\n",
        "print(f\"Number of right images: {len(right_images_path)}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27UADoghoMvG"
      },
      "outputs": [],
      "source": [
        "def getShape(images_paths):\n",
        "    shape = cv2.imread(images_paths[0]).shape\n",
        "    for image_path in images_paths:\n",
        "        image_shape=cv2.imread(image_path).shape\n",
        "        if (image_shape!=shape):\n",
        "            return \"Different image shape\"\n",
        "        else:\n",
        "            return \"Same image shape \" + str(shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "awnO0bxVoMvH",
        "outputId": "edd8a2e9-1d0a-45ca-eccf-354fa68faeb2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Same image shape (245, 200, 3)'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "getShape(left_images_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "-Ok218uJoMvH",
        "outputId": "1538822c-a643-4af2-ccd7-5be1a277d33d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Same image shape (245, 200, 3)'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "getShape(right_images_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrvKObztoMvH"
      },
      "outputs": [],
      "source": [
        "# Sort the list based on the image file names\n",
        "anchor_images = sorted(left_images_path, key=lambda x: x.split('/')[-1])\n",
        "positive_images = sorted(right_images_path, key=lambda x: x.split('/')[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MmSqz1nroMvI"
      },
      "outputs": [],
      "source": [
        "def preprocess_image(image_tensor):\n",
        "    \"\"\"\n",
        "    Preprocess the input image tensor.\n",
        "    \"\"\"\n",
        "\n",
        "    # Define the transformations\n",
        "    transform = transforms.Compose([\n",
        "    transforms.Resize(Config['TARGET_SHAPE'], antialias=True),  # Explicitly set antialias to True\n",
        "    ])\n",
        "\n",
        "    # Apply the transformations\n",
        "    image = transform(image_tensor)\n",
        "\n",
        "    return image\n",
        "\n",
        "def preprocess_triplets(anchor, positive, negative):\n",
        "    \"\"\"\n",
        "    Given the filenames corresponding to the three images, load and\n",
        "    preprocess them.\n",
        "    \"\"\"\n",
        "    return (\n",
        "        preprocess_image(anchor),\n",
        "        preprocess_image(positive),\n",
        "        preprocess_image(negative),\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Y6PeVesZoMvI"
      },
      "outputs": [],
      "source": [
        "\n",
        "class TLLDataset(Dataset):\n",
        "    def __init__(self, anchor_image_paths, positive_image_paths):\n",
        "\n",
        "        # Load images and convert to tensors\n",
        "        anchor_images = [read_image(path) for path in anchor_image_paths]\n",
        "        positive_images = [read_image(path) for path in positive_image_paths]\n",
        "\n",
        "        # Shuffle anchor and positive images to get negative images\n",
        "        negative_images = anchor_images + positive_images\n",
        "        random.shuffle(negative_images)\n",
        "\n",
        "        self.anchor_images = torch.stack(anchor_images)\n",
        "        self.positive_images = torch.stack(positive_images)\n",
        "        self.negative_images = torch.stack(negative_images)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.anchor_images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        anchor = self.anchor_images[idx]\n",
        "        positive = self.positive_images[idx]\n",
        "        negative = self.negative_images[idx]\n",
        "\n",
        "        anchor, positive, negative = preprocess_triplets(anchor, positive, negative)\n",
        "\n",
        "        return anchor, positive, negative\n",
        "\n",
        "# Create an instance of TLLDataset\n",
        "tll_dataset = TLLDataset(anchor_images, positive_images)\n",
        "\n",
        "# Determine the indices for training, validation, and testing\n",
        "image_count = len(tll_dataset)\n",
        "indices = list(range(image_count))\n",
        "\n",
        "# Define split points\n",
        "train_split = round(image_count * 0.8)\n",
        "val_split = train_split + round(image_count * 0.1)\n",
        "\n",
        "train_indices = indices[:train_split]\n",
        "val_indices = indices[train_split:val_split]\n",
        "test_indices = indices[val_split:]\n",
        "\n",
        "# Create SubsetRandomSamplers\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "val_sampler = SubsetRandomSampler(val_indices)\n",
        "test_sampler = SubsetRandomSampler(test_indices)\n",
        "\n",
        "# Create DataLoaders for training, validation, and testing\n",
        "train_loader = DataLoader(tll_dataset, batch_size=16, sampler=train_sampler, num_workers=8, drop_last=True)\n",
        "val_loader = DataLoader(tll_dataset, batch_size=16, sampler=val_sampler, num_workers=8, drop_last=True)\n",
        "test_loader = DataLoader(tll_dataset, batch_size=16, sampler=test_sampler, num_workers=8, drop_last=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPNDWwoJoMvI"
      },
      "outputs": [],
      "source": [
        "def visualize(anchor, positive, negative):\n",
        "    \"\"\"Visualize a few triplets from the supplied batches.\"\"\"\n",
        "\n",
        "    def show(ax, image_tensor):\n",
        "        # Define a transform to convert a tensor to PIL image\n",
        "        transform = transforms.ToPILImage()\n",
        "        # Convert the tensor to PIL image using the above transform\n",
        "        img = transform(image_tensor)\n",
        "\n",
        "        ax.imshow(img)\n",
        "        ax.get_xaxis().set_visible(False)\n",
        "        ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    fig, axs = plt.subplots(3, 3, figsize=(9, 9))\n",
        "\n",
        "    for i in range(3):\n",
        "        show(axs[i, 0], anchor[i])\n",
        "        show(axs[i, 1], positive[i])\n",
        "        show(axs[i, 2], negative[i])\n",
        "\n",
        "# Get one batch of data\n",
        "for anchor, positive, negative in train_loader:\n",
        "    visualize(anchor, positive, negative)\n",
        "    break  # visualize only one batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I08f24OuoMvI"
      },
      "outputs": [],
      "source": [
        "class EmbeddingNet(nn.Module):\n",
        "    def __init__(self):\n",
        "\n",
        "        super(EmbeddingNet, self).__init__()\n",
        "\n",
        "        # Load a pre-trained ResNet50 model\n",
        "        resnet = resnet50()\n",
        "\n",
        "        # Freeze all layers except the last Convolution block\n",
        "        for name, param in resnet.named_parameters():\n",
        "            if \"layer4\" not in name:\n",
        "                param.requires_grad = False\n",
        "\n",
        "        # Define the embedding network by adding a few dense layers\n",
        "        self.features = nn.Sequential(*list(resnet.children())[:-1])  # Exclude the last FC layer\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dense1 = nn.Sequential(nn.Linear(2048, 512), nn.ReLU(), SyncBatchNorm(512))\n",
        "        self.dense2 = nn.Sequential(nn.Linear(512, 256), nn.ReLU(), SyncBatchNorm(256))\n",
        "        self.output = nn.Linear(256, 256)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.dense1(x)\n",
        "        x = self.dense2(x)\n",
        "        x = self.output(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myOb0GmaoMvJ"
      },
      "outputs": [],
      "source": [
        "class DistanceLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DistanceLayer, self).__init__()\n",
        "\n",
        "    def forward(self, anchor, positive, negative):\n",
        "        ap_distance = F.pairwise_distance(anchor, positive, 2)\n",
        "        an_distance = F.pairwise_distance(anchor, negative, 2)\n",
        "        return ap_distance, an_distance\n",
        "\n",
        "class TripletMarginLoss(nn.Module):\n",
        "    def __init__(self, margin):\n",
        "        super(TripletMarginLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, ap_distance, an_distance):\n",
        "        return F.relu(ap_distance - an_distance + self.margin).mean()\n",
        "\n",
        "class SiameseNetwork(nn.Module):\n",
        "    def __init__(self, embedding_net):\n",
        "        super(SiameseNetwork, self).__init__()\n",
        "        self.embedding_net = embedding_net\n",
        "        self.distance_layer = DistanceLayer()\n",
        "\n",
        "    def forward(self, anchor, positive, negative):\n",
        "        anchor_embedding = self.embedding_net(anchor)\n",
        "        positive_embedding = self.embedding_net(positive)\n",
        "        negative_embedding = self.embedding_net(negative)\n",
        "        ap_distance, an_distance = self.distance_layer(anchor_embedding, positive_embedding, negative_embedding)\n",
        "        return ap_distance, an_distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QEsOd8NLoMvJ"
      },
      "outputs": [],
      "source": [
        "def fit(model, optimizer, train_loader, val_loader, num_epochs, margin, device, accelerator):\n",
        "    # Define the triplet loss criterion\n",
        "    criterion = TripletMarginLoss(margin)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        accelerator.print(f\"{'='*20} Epoch: {epoch+1} {'='*20}\\n\")\n",
        "\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for idx, (anchor, positive, negative) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            anchor = anchor.to(device, dtype=torch.float)\n",
        "            positive = positive.to(device, dtype=torch.float)\n",
        "            negative = negative.to(device, dtype=torch.float)\n",
        "            ap_distance, an_distance = model(anchor, positive, negative)\n",
        "            loss = criterion(ap_distance, an_distance)\n",
        "\n",
        "            accelerator.backward(loss)\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            if idx % 100 == 0:  # Print after every 100 batches\n",
        "                accelerator.print(f\"Epoch: {epoch+1}/{num_epochs}, Batch: {idx}, Training Loss: {loss.item():.4f}\")\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        accelerator.print(f\"\\nEpoch: {epoch+1} / {num_epochs}  |  Average Training Loss: {avg_train_loss:.4f}\\n\")\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for idx, (anchor, positive, negative) in enumerate(val_loader):\n",
        "                anchor = anchor.to(device, dtype=torch.float)\n",
        "                positive = positive.to(device, dtype=torch.float)\n",
        "                negative = negative.to(device, dtype=torch.float)\n",
        "                ap_distance, an_distance = model(anchor, positive, negative)\n",
        "                loss = criterion(ap_distance, an_distance)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                if idx % 10 == 0:  # Print after every 10 batches\n",
        "                    accelerator.print(f\"Epoch: {epoch+1}/{num_epochs}, Batch: {idx}, Validation Loss: {loss.item():.4f}\")\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        accelerator.print(f\"\\nEpoch: {epoch+1} / {num_epochs}  |  Average Validation Loss: {avg_val_loss:.4f}\\n\")\n",
        "\n",
        "        # Save the model\n",
        "        accelerator.wait_for_everyone()\n",
        "        model = accelerator.unwrap_model(model)\n",
        "        model_path = f\"epoch_{epoch+1}_model.pth\"\n",
        "        accelerator.save(model, model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmvgcF0loMvJ"
      },
      "outputs": [],
      "source": [
        "def training_loop(train_loader, val_loader, Config):\n",
        "    accelerator = Accelerator()\n",
        "    device = accelerator.device\n",
        "\n",
        "    embedding_net = EmbeddingNet()\n",
        "    model = SiameseNetwork(embedding_net)\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=Config['LR'])\n",
        "\n",
        "    model, optimizer, train_dataloader, val_loader = accelerator.prepare(\n",
        "        model, optimizer, train_loader, val_loader\n",
        "    )\n",
        "\n",
        "    # Start training\n",
        "    fit(\n",
        "            model = model,\n",
        "            optimizer = optimizer,\n",
        "            train_loader = train_loader,\n",
        "            val_loader = val_loader,\n",
        "            num_epochs = Config['NUM_EPOCHS'],\n",
        "            margin = Config['MARGIN'],\n",
        "            device = device,\n",
        "            accelerator = accelerator,\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGWXf6PioMvJ"
      },
      "outputs": [],
      "source": [
        "args = (train_loader, val_loader, Config)\n",
        "notebook_launcher(training_loop, args, num_processes=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akYRgTncoMvJ"
      },
      "outputs": [],
      "source": [
        "def compute_similarity(model, anchor, positive, negative):\n",
        "    with torch.no_grad():\n",
        "        anchor_embedding = model(anchor).squeeze(0).view(-1)\n",
        "        positive_embedding = model(positive).squeeze(0).view(-1)\n",
        "        negative_embedding = model(negative).squeeze(0).view(-1)\n",
        "\n",
        "    positive_similarity = F.cosine_similarity(anchor_embedding.unsqueeze(0), positive_embedding.unsqueeze(0))\n",
        "    negative_similarity = F.cosine_similarity(anchor_embedding.unsqueeze(0), negative_embedding.unsqueeze(0))\n",
        "\n",
        "    return positive_similarity.item(), negative_similarity.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Dk5ZKqGoMvK"
      },
      "outputs": [],
      "source": [
        "def inference_loop(model, device, anchor, positive, negative, is_pretrained=True):\n",
        "    # Ensure the input tensors are on the correct device\n",
        "    anchor = anchor.to(device, dtype=torch.float)\n",
        "    positive = positive.to(device, dtype=torch.float)\n",
        "    negative = negative.to(device, dtype=torch.float)\n",
        "\n",
        "    model = model.to(device)\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    pos_sim, neg_sim = compute_similarity(model, anchor, positive, negative)\n",
        "\n",
        "    model_type = \"Pretrained Model\" if is_pretrained else \"Fine-tuned Model\"\n",
        "\n",
        "    print(f\"\\nFor {model_type}:\")\n",
        "    print(\"Positive similarity:\", pos_sim)\n",
        "    print(\"Negative similarity:\", neg_sim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FW4LAOZQoMvK"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Extract a batch from the test_loader\n",
        "anchor, positive, negative = next(iter(test_loader))\n",
        "visualize(anchor, positive, negative)\n",
        "\n",
        "# For pretrained model\n",
        "# Load the pretrained ResNet model\n",
        "resnet_model = resnet50()\n",
        "# Remove the classification head to get embeddings\n",
        "embedding_resnet = nn.Sequential(*list(resnet_model.children())[:-1])\n",
        "inference_loop(embedding_resnet, device, anchor, positive, negative, is_pretrained=True)\n",
        "\n",
        "# For fine-tuned model\n",
        "model = torch.load(\"epoch_10_model.pth\")\n",
        "embedding_finetuned = model.embedding_net\n",
        "inference_loop(embedding_finetuned, device, anchor, positive, negative, is_pretrained=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26i02AhWoMvK"
      },
      "source": [
        "### Pretrained Model Results:\n",
        "\n",
        "The pretrained model was trained on a large dataset (likely ImageNet) to recognize a wide range of classes. When you use it as a feature extractor (embedding generator), it provides embeddings that capture generic visual features. This is evident from the results:\n",
        "- The positive pair, which should be similar, has a high cosine similarity.\n",
        "- The negative pair, which should be dissimilar, also has a relatively high cosine similarity. This indicates that the embeddings from the pretrained model might not be discriminative enough for your specific task.\n",
        "\n",
        "### Fine-tuned Model Results:\n",
        "\n",
        "The fine-tuned model has been trained specifically for the task of differentiating between the anchor and positive/negative pairs. The results are more aligned with what one would expect:\n",
        "- The positive pair has a higher cosine similarity than the negative pair, which is the desired outcome."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qKh_frX9sHKW"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}